{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"OzafjWQP18sH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674750247602,"user_tz":-360,"elapsed":49014,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}},"outputId":"33b87117-4f54-42c7-8187-4ec441aef51a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#**Pretrained**"],"metadata":{"id":"2c5vj0zDUCyJ"}},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n","WORD2VEC_MODEL = '/content/drive/MyDrive/Sunjare CSE499/Copy of GoogleNews-vectors-negative300.bin.gz'\n","word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)"],"metadata":{"id":"J5GTyANgUBl7","executionInfo":{"status":"aborted","timestamp":1674750179739,"user_tz":-360,"elapsed":152,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dz3twXOr355c"},"source":["##**Importing Libraries**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQmR-ESc3-rR","executionInfo":{"status":"aborted","timestamp":1674750179743,"user_tz":-360,"elapsed":155,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["from keras.layers.core import Dense, SpatialDropout1D\n","from keras.layers.convolutional import Conv1D\n","from tensorflow.keras.layers import Embedding, Flatten, Activation\n","from keras.layers.pooling import GlobalMaxPooling1D, MaxPooling1D\n","from tensorflow.keras.models import Sequential\n","from keras.optimizers import SGD\n","from keras_preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import LSTM\n","from keras.layers import Dropout\n","from keras.layers import BatchNormalization\n","from keras.initializers import Constant\n","from keras.utils import np_utils\n","from keras.initializers import Constant\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import collections\n","import nltk\n","import numpy as np\n","import codecs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6Lva4CGgKlp","executionInfo":{"status":"aborted","timestamp":1674750179745,"user_tz":-360,"elapsed":156,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AiEeZ5nLgQEQ","executionInfo":{"status":"aborted","timestamp":1674750179746,"user_tz":-360,"elapsed":156,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["\n","from nltk.corpus import stopwords\n","\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","stop_words = set(stopwords.words('english'))\n","not_stopwords = {'not','no'} \n","final_stop_words= set([word for word in stop_words if word not in not_stopwords])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWjMP3a8Nf9E","executionInfo":{"status":"aborted","timestamp":1674750179755,"user_tz":-360,"elapsed":165,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["np.random.seed(42)\n","\n","# train_review = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/SST2_reviews_train.txt'\n","# train_sentiment = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/SST2 sentiment_train.txt'\n","# test_review = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/SST2_reviews_test.txt'\n","# test_sentiment = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/SST2_sentiment_test.txt'\n","\n","\n","# train_review = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/1000 SST2_reviews_train.txt'\n","# train_sentiment = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/1000 SST2_sentiment_train.txt'\n","# test_review = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/1000 SST2_reviews_test.txt'\n","# test_sentiment = '/content/drive/MyDrive/Sunjare CSE499/Dataset/SST2/1000 SST2_sentiment_test.txt'\n","\n","\n","# train_review = '/content/drive/MyDrive/Sunjare CSE499/hudai test/new_reviews.txt'\n","# train_sentiment = '/content/drive/MyDrive/Sunjare CSE499/hudai test/new_sentiment.txt'\n","# test_review = '/content/drive/MyDrive/Sunjare CSE499/hudai test/SST2_reviews_test.txt'\n","# test_sentiment = '/content/drive/MyDrive/Sunjare CSE499/hudai test/SST2_sentiment_test.txt'\n","\n","\n","train_review = '/content/drive/MyDrive/Sunjare CSE499/Dataset/GLUE Benchmark Dataset/RTE/500/train_reviews.txt'\n","train_sentiment = '/content/drive/MyDrive/Sunjare CSE499/Dataset/GLUE Benchmark Dataset/RTE/500/train_sentiments.txt'\n","test_review = '/content/drive/MyDrive/Sunjare CSE499/Dataset/GLUE Benchmark Dataset/RTE/500/test_reviews.txt'\n","test_sentiment = '/content/drive/MyDrive/Sunjare CSE499/Dataset/GLUE Benchmark Dataset/RTE/500/test_sentiments.txt'\n","\n","VOCAB_SIZE = 15000\n","EMBED_SIZE = 300\n","NUM_FILTERS = 256\n","NUM_WORDS = 3\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 60\n"]},{"cell_type":"markdown","metadata":{"id":"BXEdOuTCKzm6"},"source":["##**Dataset Preparation and Cleaning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqZ9rg50_efL","executionInfo":{"status":"aborted","timestamp":1674750179757,"user_tz":-360,"elapsed":166,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["counter = collections.Counter()\n","maxlen = 0\n","train_xs_word2vec, train_ys_word2vec = [], []\n","\n","\n","train_fin_review = codecs.open(train_review, \"r\", encoding='utf-8')\n","for line in train_fin_review:\n","    sent = line #Stripping the dataset based on tab. That is stripping label from sentence\n","\n","    print(\"Sentence: \",sent)\n","\n","    words = [x.lower() for x in nltk.word_tokenize(sent)] #lowering the sentence and tokenizing\n","    print(\"Actual:\")\n","    print(words)\n","\n","    wordsExcludingPunctuationMarks=[word for word in words if word.isalnum()]\n","    print(\"Truncating punctuation:\")\n","    print(wordsExcludingPunctuationMarks)\n","\n","    wordsExcludingStopWords = [word for word in wordsExcludingPunctuationMarks if word not in final_stop_words]\n","    print(\"Truncating StopWords:\")\n","    print(wordsExcludingStopWords)\n","\n","\n","\n","\n","    afterLemmatizing = [lemmatizer.lemmatize(token) for token in wordsExcludingStopWords]\n","    afterLemmatizing = [lemmatizer.lemmatize(token, \"v\") for token in afterLemmatizing]\n","    print(\"After Lemmatizing:\")\n","    print(afterLemmatizing)\n","\n","\n","\n","    if len(afterLemmatizing) > maxlen: #For calculating the maximum number of words in a sentence\n","        maxlen = len(afterLemmatizing) \n","    for afterLemmitize in afterLemmatizing:\n","        counter[afterLemmitize] += 1 #Putting the frequency of each  word in a dictionary\n","    print(\"***************************************************************************************\")\n","\n","    train_xs_word2vec.append(afterLemmatizing )\n","\n","train_fin_review.close()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkjCnRPCjVaZ","executionInfo":{"status":"aborted","timestamp":1674750179758,"user_tz":-360,"elapsed":166,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["train_fin_sentiment = codecs.open(train_sentiment, \"r\", encoding='utf-8')\n","\n","for line in train_fin_sentiment:\n","  train_ys_word2vec.append(int(line))\n","  \n","train_fin_sentiment.close()\n","\n","train_ys_word2vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2gTrIY7nVJT","executionInfo":{"status":"aborted","timestamp":1674750179760,"user_tz":-360,"elapsed":167,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["train_xs_word2vec\n","print(len(train_xs_word2vec))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouq1Hdp3AOM_","executionInfo":{"status":"aborted","timestamp":1674750179767,"user_tz":-360,"elapsed":173,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["print(\"Maximum number of word in a sentence: \",maxlen)\n","print(\"Frequency of each words: \")\n","counter"]},{"cell_type":"markdown","metadata":{"id":"-c6DSiTvPiS_"},"source":["##**Generating Word2index**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1bWOEcUIdR8","executionInfo":{"status":"aborted","timestamp":1674750179769,"user_tz":-360,"elapsed":150,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["word2index = collections.defaultdict(int) \n","for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n","    word2index[word[0]] = wid + 1\n","\n","word2index  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFN_V9rRItg6","executionInfo":{"status":"aborted","timestamp":1674750179770,"user_tz":-360,"elapsed":150,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["vocab_sz = len(word2index) + 1\n","print(\"Number of Vocabularies: \",vocab_sz)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3VN5LVPvlSR","executionInfo":{"status":"aborted","timestamp":1674750179772,"user_tz":-360,"elapsed":151,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["train_xs, train_ys = [], []\n","test_xs, test_ys = [], []\n","\n","train_fin_review = codecs.open(train_review, \"r\", encoding='utf-8')\n","for line in train_fin_review:\n","    sent = line #Stripping the dataset based on tab. That is stripping label from sentence\n","\n","    words = [x.lower() for x in nltk.word_tokenize(sent)] #lowering the sentence and tokenizing\n","    wordsExcludingPunctuationMarks=[word for word in words if word.isalnum()]\n","    wordsExcludingStopWords = [word for word in wordsExcludingPunctuationMarks if word not in final_stop_words]\n","\n","    afterLemmatizing = [lemmatizer.lemmatize(token) for token in wordsExcludingStopWords]\n","    afterLemmatizing = [lemmatizer.lemmatize(token, \"v\") for token in afterLemmatizing]\n","\n","    wids = [word2index[word] for word in afterLemmatizing]\n","    train_xs.append(wids)\n","\n","train_ys = train_ys_word2vec\n","train_fin_review.close()\n","\n","\n","\n","#For Validation\n","test_fin_review = codecs.open(test_review, \"r\", encoding='utf-8')\n","for line in test_fin_review:\n","    sent = line #Stripping the dataset based on tab. That is stripping label from sentence\n","\n","    words = [x.lower() for x in nltk.word_tokenize(sent)] #lowering the sentence and tokenizing\n","    wordsExcludingPunctuationMarks=[word for word in words if word.isalnum()]\n","    wordsExcludingStopWords = [word for word in wordsExcludingPunctuationMarks if word not in final_stop_words]\n","\n","    afterLemmatizing = [lemmatizer.lemmatize(token) for token in wordsExcludingStopWords]\n","    afterLemmatizing = [lemmatizer.lemmatize(token, \"v\") for token in afterLemmatizing]\n","\n","    wids = [word2index[word] for word in afterLemmatizing]\n","    test_xs.append(wids)\n","\n","test_fin_review.close()\n","\n","\n","\n","#For Validation\n","test_fin_sentiment = codecs.open(test_sentiment, \"r\", encoding='utf-8')\n","\n","for line in test_fin_sentiment:\n","  test_ys.append(int(line))\n","  \n","test_fin_sentiment.close()\n","\n","test_ys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQrVOG84ntEL","executionInfo":{"status":"aborted","timestamp":1674750179773,"user_tz":-360,"elapsed":146,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["for i in range(len(train_xs)):\n","  print(train_xs_word2vec[i])\n","  print(train_xs[i])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5i8pOYlqXVp","executionInfo":{"status":"aborted","timestamp":1674750179787,"user_tz":-360,"elapsed":159,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["train_ys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFRuLHJ3HH_9","executionInfo":{"status":"aborted","timestamp":1674750179789,"user_tz":-360,"elapsed":160,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["Xtrain = pad_sequences(train_xs, maxlen=maxlen)\n","Ytrain = np_utils.to_categorical(train_ys)\n","\n","Xtest = pad_sequences(test_xs, maxlen=maxlen)\n","Ytest = np_utils.to_categorical(test_ys)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"232lagtJHPH0","executionInfo":{"status":"aborted","timestamp":1674750179790,"user_tz":-360,"elapsed":160,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["Xtrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulz18LRKHPyU","executionInfo":{"status":"aborted","timestamp":1674750179792,"user_tz":-360,"elapsed":161,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["print(Ytrain[8])"]},{"cell_type":"markdown","metadata":{"id":"05kdOS98lDAn"},"source":["#**Word2Vec**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EzgKHhENlId1","executionInfo":{"status":"aborted","timestamp":1674750179793,"user_tz":-360,"elapsed":161,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# import gensim\n","# model=gensim.models.Word2Vec(sentences=train_xs_word2vec,size=300,window=5,min_count=1, sg=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSv4xiCylRyt","executionInfo":{"status":"aborted","timestamp":1674750179794,"user_tz":-360,"elapsed":161,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# model.train(train_xs_word2vec,epochs=500,total_examples=len(train_xs_word2vec))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfG98zw7n2kN","executionInfo":{"status":"aborted","timestamp":1674750179795,"user_tz":-360,"elapsed":161,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# model.wv.most_similar(\"bad\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOFYjbcQn4VE","executionInfo":{"status":"aborted","timestamp":1674750179797,"user_tz":-360,"elapsed":162,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# model.wv.most_similar(\"good\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVieWEGJn63e","executionInfo":{"status":"aborted","timestamp":1674750179799,"user_tz":-360,"elapsed":164,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# vocab=model.wv.vocab\n","# print(\"The total number of words are : \",len(vocab))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8AcbuBBoCNm","executionInfo":{"status":"aborted","timestamp":1674750179800,"user_tz":-360,"elapsed":164,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# vocab=list(vocab.keys())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXQxvHcLoD2B","executionInfo":{"status":"aborted","timestamp":1674750179802,"user_tz":-360,"elapsed":165,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# for word in vocab:\n","#   print(word)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XS64uO6goGBK","executionInfo":{"status":"aborted","timestamp":1674750179804,"user_tz":-360,"elapsed":166,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# word_vec_dict={}\n","# for word in vocab:\n","#   word_vec_dict[word]=model.wv.get_vector(word)\n","# print(\"The no of key-value pairs : \",len(word_vec_dict)) \n","# print(word_vec_dict.keys())\n","\n","# print(\"***\")\n","\n","# res = list(word_vec_dict.values())[0]\n","# print(str(res))"]},{"cell_type":"markdown","metadata":{"id":"QlU2Q9RdoKPX"},"source":["#**Word Embeddings**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GjckiH9oMx3","executionInfo":{"status":"aborted","timestamp":1674750179806,"user_tz":-360,"elapsed":168,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# # now creating the embedding matrix\n","# embed_dim = 300\n","# from nltk import word_tokenize,sent_tokenize # tokenizing\n","\n","\n","# embed_matrix=np.zeros(shape=(vocab_sz,embed_dim))\n","\n","# print(embed_matrix.shape)\n","\n","# for word,i in word2index.items():\n","#   embed_vector=word_vec_dict.get(word)\n","#   print(str(word) + \"***\" + str(i))\n","#   if embed_vector is not None and embed_vector.size!=0:  # word is in the vocabulary learned by the w2v model\n","#     embed_matrix[i]=embed_vector\n","#     print(\"--->\"  + str(word) + \"***\" + str(i))\n","#     print(\"\\n\")\n","#   # if word is not found then embed_vector corressponding to that vector will stay zero."]},{"cell_type":"markdown","source":["#**For Pretrained Word2Vec**\n","\n","\n","\n"],"metadata":{"id":"4cuj2YaQkl25"}},{"cell_type":"code","source":["embed_matrix = np.zeros((vocab_sz, EMBED_SIZE))\n","for word, index in word2index.items():\n","    try:\n","        embed_matrix[index, :] = word2vec[word]\n","    except KeyError:\n","        pass"],"metadata":{"id":"hAYEKy5qkk0a","executionInfo":{"status":"aborted","timestamp":1674750179807,"user_tz":-360,"elapsed":168,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LbpPMAWWnnz9"},"source":["#**Test Train Split**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otC0GcUXCcX6","executionInfo":{"status":"aborted","timestamp":1674750179808,"user_tz":-360,"elapsed":169,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["print(Xtrain.shape)\n","print(Ytrain.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGo5lqNQHVwr","executionInfo":{"status":"aborted","timestamp":1674750179809,"user_tz":-360,"elapsed":170,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)"]},{"cell_type":"markdown","metadata":{"id":"QO241JJYHT2W"},"source":["#**Learning Rate Range Finder**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dznlNOprnwVZ","executionInfo":{"status":"aborted","timestamp":1674750179810,"user_tz":-360,"elapsed":170,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# Github Link: https://gist.github.com/WittmannF/c55ed82d27248d18799e2be324a79473\n","from keras.callbacks import Callback\n","import keras.backend as K\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import optimize\n","from scipy.ndimage.filters import gaussian_filter1d\n","\n","class LRFinder(Callback):\n","    \"\"\"\n","    Up-to date version: https://github.com/WittmannF/LRFinder\n","    \"\"\"\n","    def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None, \n","                 reload_weights=True, batches_lr_update=10):\n","        self.min_lr = min_lr\n","        self.max_lr = max_lr\n","        self.mom = mom\n","        self.reload_weights = reload_weights\n","        self.batches_lr_update = batches_lr_update\n","        if stop_multiplier is None:\n","            self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n","                                                       # 10 if mom=0\n","        else:\n","            self.stop_multiplier = stop_multiplier\n","        \n","    def on_train_begin(self, logs={}):\n","        p = self.params\n","        try:\n","            n_iterations = p['epochs']*p['samples']//p['batch_size']\n","        except:\n","            n_iterations = p['steps']*p['epochs']\n","            \n","        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n","                                           num=n_iterations//self.batches_lr_update+1)\n","        self.losses=[]\n","        self.iteration=0\n","        self.best_loss=0\n","        \n","        self.per_epoch_loss = []\n","        self.accuracy_list = []\n","\n","        if self.reload_weights:\n","            self.model.save_weights('tmp.hdf5')\n","        \n","\n","\n","\n","\n","    def get_percentage_diff(previous, current):\n","      try:\n","          percentage = abs(previous - current)/max(previous, current) * 100\n","      except ZeroDivisionError:\n","          percentage = float('inf')\n","      return percentage    \n","\n","    \n","    def on_batch_end(self, batch, logs={}):\n","        loss = logs.get('loss')\n","        accuracy = logs.get('accuracy')\n","\n","        if self.iteration!=0: # Make loss smoother using momentum\n","            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n","        \n","        if self.iteration==0 or loss < self.best_loss: \n","                self.best_loss = loss\n","                \n","        if self.iteration%self.batches_lr_update==0: # Evaluate each lr over 5 epochs\n","            \n","            if self.reload_weights:\n","                self.model.load_weights('tmp.hdf5')\n","          \n","            lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n","            K.set_value(self.model.optimizer.lr, lr)\n","\n","            self.losses.append(loss) \n","            self.accuracy_list.append(accuracy)         \n","\n","        if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n","            self.model.stop_training = True\n","        \n","        self.iteration += 1\n","\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","      \n","          loss = logs.get('loss') * 100.0\n","\n","          if len(self.per_epoch_loss)>0:\n","            previous_minimum_loss = min(self.per_epoch_loss)\n","\n","          self.per_epoch_loss.append(loss)\n","          \n","\n","          if epoch>0:\n","            print(\"\\n *** Epoch \" + str(epoch) + \" Completed ***\")\n","            print(\"Present loss in epoch \"+str(epoch)  +\": \"+ str(loss))\n","            print(\"Minimum Loss: \" + str(min(self.per_epoch_loss)))\n","\n","\n","            percentage_icrease = abs(previous_minimum_loss - loss)/max(previous_minimum_loss, loss) * 100\n","            if loss > previous_minimum_loss:\n","              print(\"Loss Increased By: \" + str(percentage_icrease))  \n","\n","              if percentage_icrease>3.0 and epoch>5:\n","                print(\"\\n Alret!!! Loss Increasing, Model Will Stop Now \\n\")\n","                self.model.stop_training = True\n","\n","            else:\n","              print(\"Loss Decresed By: \" + str(percentage_icrease))\n","              \n","        \n","          else:\n","            print(\"\\n *** Epoch \" + str(epoch) + \" Completed ***\")\n","\n","          \n","\n","    \n","    def on_train_end(self, logs=None):\n","        if self.reload_weights:\n","                self.model.load_weights('tmp.hdf5')\n","\n","        \n","        minpos = self.losses.index(min(self.losses))\n","        global_minima = self.learning_rates[:len(self.losses)][minpos]\n","        print(\"Global minima from list:\",global_minima)\n","\n","\n","\n","        plt.figure(figsize=(20, 6))\n","\n","\n","        # plt.plot(self.learning_rates[:len(self.losses)],self.losses,color=\"orange\",label=\"Loss vs LR\")\n","        plt.xlabel(\"Learning Rate\")\n","        plt.ylabel(\"Loss\")\n","\n","\n","        ysmoothed = gaussian_filter1d(self.losses, sigma=2)\n","        minpos_graph = np.argmin(ysmoothed)\n","        plt.plot(self.learning_rates[:len(self.losses)], ysmoothed)\n","        plt.plot(self.learning_rates[:len(self.losses)][minpos_graph], self.losses[minpos_graph], 'go', label=\"Minima\")\n","\n","\n","        draw_polynomial = np.poly1d(np.polyfit(self.learning_rates[:len(self.losses)], ysmoothed, 10))\n","        polyline = np.linspace(self.min_lr, max(self.learning_rates[:len(self.losses)]), 300)\n","        plt.plot(polyline, draw_polynomial(polyline), color='purple')\n","        # plt.plot(self.learning_rates[:len(self.losses)][minpos], min(self.losses), 'ro', label=\"Minima\")\n"," \n","\n","\n","        roots = [np.real(z) for z in draw_polynomial.deriv().roots \n","                if np.imag(z)==0 and self.min_lr<=np.real(z)<=max(self.learning_rates[:len(self.losses)])]\n","\n","        \n","        roots.sort()\n","        minimum_roots = []\n","\n","        for root in roots:\n","          second_derivative = draw_polynomial.deriv(2)(root)\n","          if second_derivative>0:\n","            print(\"its minimum: \",root)\n","            # it's a minimum \n","            minimum_roots.append((draw_polynomial(root),root)) \n","            \n","\n","          elif second_derivative<0:\n","            print(\"its maximum: \", root)\n","            \n","            # it's a maximum\n","          else:\n","            print(\"its point of flexure: \",root)\n","            # it's a point of flexure\n","\n","\n","        \n","        # Sort the list of tuples by the first element of each tuple\n","        minimum_loss_of_polyfit = sorted(minimum_roots)\n","        root = minimum_loss_of_polyfit[0][1] \n","        f = open(\"global_minima.txt\", \"a\")\n","        f.truncate(0)\n","        f.write(str(root))\n","        f.close()\n","\n","        plt.plot([root,root],[draw_polynomial(root),draw_polynomial(root) + 0.01], 'r--')\n","        base_lr = (70.0 * root) / 100.0\n","        plt.plot([base_lr ,base_lr ],[draw_polynomial(root),draw_polynomial(root) + 0.01], 'r--')\n","\n","\n","\n","        plt.xscale('log')\n","        plt.show()\n","\n","        print(self.accuracy_list)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","#         # Github Link: https://gist.github.com/WittmannF/c55ed82d27248d18799e2be324a79473\n","# from keras.callbacks import Callback\n","# import keras.backend as K\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","# from scipy import optimize\n","# from scipy.ndimage.filters import gaussian_filter1d\n","\n","# class LRFinder(Callback):\n","#     \"\"\"\n","#     Up-to date version: https://github.com/WittmannF/LRFinder\n","#     \"\"\"\n","#     def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None, \n","#                  reload_weights=True, batches_lr_update=5):\n","#         self.min_lr = min_lr\n","#         self.max_lr = max_lr\n","#         self.mom = mom\n","#         self.reload_weights = reload_weights\n","#         self.batches_lr_update = batches_lr_update\n","#         if stop_multiplier is None:\n","#             self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n","#                                                        # 10 if mom=0\n","#         else:\n","#             self.stop_multiplier = stop_multiplier\n","        \n","#     def on_train_begin(self, logs={}):\n","#         p = self.params\n","#         try:\n","#             n_iterations = p['epochs']*p['samples']//p['batch_size']\n","#         except:\n","#             n_iterations = p['steps']*p['epochs']\n","            \n","#         self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n","#                                            num=n_iterations//self.batches_lr_update+1)\n","#         self.losses=[]\n","#         self.iteration=0\n","#         self.best_loss=0\n","#         if self.reload_weights:\n","#             self.model.save_weights('tmp.hdf5')\n","        \n","    \n","#     def on_batch_end(self, batch, logs={}):\n","#         loss = logs.get('loss')\n","        \n","#         if self.iteration!=0: # Make loss smoother using momentum\n","#             loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n","        \n","#         if self.iteration==0 or loss < self.best_loss: \n","#                 self.best_loss = loss\n","                \n","#         if self.iteration%self.batches_lr_update==0: # Evaluate each lr over 5 epochs\n","            \n","#             if self.reload_weights:\n","#                 self.model.load_weights('tmp.hdf5')\n","          \n","#             lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n","#             K.set_value(self.model.optimizer.lr, lr)\n","\n","#             self.losses.append(loss)            \n","\n","#         if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n","#             self.model.stop_training = True\n","                \n","#         self.iteration += 1\n","\n","\n","    \n","#     def on_train_end(self, logs=None):\n","#         if self.reload_weights:\n","#                 self.model.load_weights('tmp.hdf5')\n","\n","        \n","#         minpos = self.losses.index(min(self.losses))\n","#         global_minima = self.learning_rates[:len(self.losses)][minpos]\n","#         print(\"Global minima from list:\",global_minima)\n","\n","\n","\n","#         plt.figure(figsize=(20, 6))\n","\n","\n","#         plt.plot(self.learning_rates[:len(self.losses)],self.losses,color=\"orange\",label=\"Loss vs LR\")\n","#         plt.plot(self.learning_rates[:len(self.losses)][minpos], min(self.losses), 'ro', label=\"Minima\")\n","#         plt.xlabel(\"Learning Rate\")\n","#         plt.ylabel(\"Loss\")\n","\n","\n","#         plt.xscale('log')\n","#         plt.show()\n","\n","#         f = open(\"global_minima.txt\", \"a\")\n","#         f.truncate(0)\n","#         f.write(str(global_minima))\n","#         f.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3CmLNhEpJIJ","executionInfo":{"status":"aborted","timestamp":1674750179812,"user_tz":-360,"elapsed":172,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# lr_finder = LRFinder(min_lr=0.0000001, max_lr=0.00001) #Low Learning Rate\n","lr_finder = LRFinder(min_lr=0.00001, max_lr=0.001)     #Medium Learning Rate\n","# lr_finder = LRFinder(min_lr=0.001, max_lr=0.1)         #High Learning Rate"]},{"cell_type":"markdown","metadata":{"id":"DWZb4JAgMrRZ"},"source":["#**Learning Rate Scheduler**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Z4XwprwM1rh","executionInfo":{"status":"aborted","timestamp":1674750179814,"user_tz":-360,"elapsed":173,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["from keras.callbacks import LearningRateScheduler\n","lr_sched = LearningRateScheduler(lambda epoch: 1e-4 * (0.90 ** np.floor(epoch / 2)))"]},{"cell_type":"markdown","metadata":{"id":"n3Kzj1nY2uFl"},"source":["#**EarlyStopping**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkozlvpN22e-","executionInfo":{"status":"aborted","timestamp":1674750179815,"user_tz":-360,"elapsed":174,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","early_stopping = EarlyStopping(\n","    monitor=\"val_accuracy\",\n","    min_delta=0,\n","    patience=25,\n","    verbose=0,\n","    mode=\"auto\",\n","    baseline=None,\n","    restore_best_weights=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7NjwgdeUfex","executionInfo":{"status":"aborted","timestamp":1674750179817,"user_tz":-360,"elapsed":176,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["import os\n","import random as rn \n","import tensorflow as tf\n","\n","def reproduceResult():\n","  seed_value= 0\n","\n","  \n","  with tf.device(\"/cpu:0\"):\n","    ...\n","\n","\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  np.random.seed(0)\n","  rn.seed(0)\n","\n","\n","  session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, \n","                                          inter_op_parallelism_threads=1)\n","\n","\n","  tf.compat.v1.set_random_seed(seed_value)\n","  sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","  tf.compat.v1.keras.backend.set_session(sess)\n","  tf.compat.v1.keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{"id":"RM7j0GMT_KFx"},"source":["#**Attention Layer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BLA42Dy_Mww","executionInfo":{"status":"aborted","timestamp":1674750179819,"user_tz":-360,"elapsed":177,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["!pip install keras_self_attention\n","import keras\n","from keras_self_attention import SeqSelfAttention"]},{"cell_type":"markdown","metadata":{"id":"QXhJy_laZqDF"},"source":["#**Bi-LSTM + CNN + BiLSTM**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0oq3HONZwH1","executionInfo":{"status":"aborted","timestamp":1674750179821,"user_tz":-360,"elapsed":179,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["from keras.layers import Bidirectional\n","\n","modelHybrid=Sequential()\n","modelHybrid.add(Embedding(vocab_sz, 300, input_length=maxlen, embeddings_initializer=Constant(embed_matrix)))\n","\n","modelHybrid.add(Bidirectional(LSTM(256,return_sequences=True)))\n","modelHybrid.add(SeqSelfAttention(attention_activation='relu'))\n","\n","modelHybrid.add(SpatialDropout1D(0.2))\n","modelHybrid.add(Conv1D(filters=128, kernel_size=NUM_WORDS, activation=\"relu\"))\n","modelHybrid.add(MaxPooling1D())\n","\n","modelHybrid.add(Bidirectional(LSTM(256,return_sequences=True)))\n","modelHybrid.add(Dropout(0.2))\n","modelHybrid.add(Bidirectional(LSTM(128)))\n","modelHybrid.add(Dropout(0.2))\n","modelHybrid.add(Dense(128,activation='relu'))\n","modelHybrid.add(Dropout(0.2))\n","modelHybrid.add(Dense(256,activation='relu'))\n","modelHybrid.add(Dropout(0.2))\n","modelHybrid.add(Dense(512,activation='relu'))\n","modelHybrid.add(Dropout(0.2))\n","modelHybrid.add(BatchNormalization())\n","modelHybrid.add(Dense(2,activation='softmax'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8cqXu0-9bbiw","executionInfo":{"status":"aborted","timestamp":1674750179822,"user_tz":-360,"elapsed":179,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["\n","import tensorflow as tf\n","\n","modelHybrid.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","# modelHybrid.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n","\n","print(modelHybrid.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qraqLr0sbgSB","executionInfo":{"status":"aborted","timestamp":1674750179823,"user_tz":-360,"elapsed":180,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["history = modelHybrid.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,validation_data=(Xtest, Ytest), callbacks=[lr_finder])"]},{"cell_type":"markdown","metadata":{"id":"5Az1z4VCpz0x"},"source":["#**CyclicLearningRate Scheduler**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RNYT0TSqQ_c","executionInfo":{"status":"aborted","timestamp":1674750179825,"user_tz":-360,"elapsed":181,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# Github link: https://github.com/bckenstler/CLR\n","from tensorflow.keras.callbacks import *\n","from tensorflow.keras import backend as K\n","import numpy as np\n","\n","class CyclicLR(Callback):\n","    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n","    The method cycles the learning rate between two boundaries with\n","    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n","    The amplitude of the cycle can be scaled on a per-iteration or \n","    per-cycle basis.\n","    This class has three built-in policies, as put forth in the paper.\n","    \"triangular\":\n","        A basic triangular cycle w/ no amplitude scaling.\n","    \"triangular2\":\n","        A basic triangular cycle that scales initial amplitude by half each cycle.\n","    \"exp_range\":\n","        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n","        cycle iteration.\n","    For more detail, please see paper.\n","    \n","    # Example\n","        ```python\n","            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n","                                step_size=2000., mode='triangular')\n","            model.fit(X_train, Y_train, callbacks=[clr])\n","        ```\n","    \n","    Class also supports custom scaling functions:\n","        ```python\n","            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n","            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n","                                step_size=2000., scale_fn=clr_fn,\n","                                scale_mode='cycle')\n","            model.fit(X_train, Y_train, callbacks=[clr])\n","        ```    \n","    # Arguments\n","        base_lr: initial learning rate which is the\n","            lower boundary in the cycle.\n","        max_lr: upper boundary in the cycle. Functionally,\n","            it defines the cycle amplitude (max_lr - base_lr).\n","            The lr at any cycle is the sum of base_lr\n","            and some scaling of the amplitude; therefore \n","            max_lr may not actually be reached depending on\n","            scaling function.\n","        step_size: number of training iterations per\n","            half cycle. Authors suggest setting step_size\n","            2-8 x training iterations in epoch.\n","        mode: one of {triangular, triangular2, exp_range}.\n","            Default 'triangular'.\n","            Values correspond to policies detailed above.\n","            If scale_fn is not None, this argument is ignored.\n","        gamma: constant in 'exp_range' scaling function:\n","            gamma**(cycle iterations)\n","        scale_fn: Custom scaling policy defined by a single\n","            argument lambda function, where \n","            0 <= scale_fn(x) <= 1 for all x >= 0.\n","            mode paramater is ignored \n","        scale_mode: {'cycle', 'iterations'}.\n","            Defines whether scale_fn is evaluated on \n","            cycle number or cycle iterations (training\n","            iterations since start of cycle). Default is 'cycle'.\n","    \"\"\"\n","\n","    def __init__(self, base_lr, max_lr, step_size, mode,\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1/(2.**(x-1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma**(x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","        \n","    def clr(self):\n","        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n","        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n","        \n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())        \n","            \n","    def on_batch_end(self, epoch, logs=None):\n","        \n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        self.history.setdefault('iterations', []).append(self.trn_iterations)\n","\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","        \n","        K.set_value(self.model.optimizer.lr, self.clr())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuCK2obtq2oR","executionInfo":{"status":"aborted","timestamp":1674750179826,"user_tz":-360,"elapsed":182,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["with open('/content/global_minima.txt', 'r') as file:\n","    global_minima = file.read()\n","\n","global_minima = float(global_minima)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jP6oOo_PQ_x","executionInfo":{"status":"aborted","timestamp":1674750179827,"user_tz":-360,"elapsed":183,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["clr_step_size = int(4 * (len(Xtrain)/BATCH_SIZE))\n","base_lr = (70.0 * global_minima) / 100.0\n","max_lr = global_minima \n","mode='exp_range'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0tAhCiYq5kA","executionInfo":{"status":"aborted","timestamp":1674750179829,"user_tz":-360,"elapsed":184,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["clr = CyclicLR(base_lr=base_lr, max_lr=max_lr, step_size=clr_step_size, mode=mode)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZiezLcRwq_Je","executionInfo":{"status":"aborted","timestamp":1674750179832,"user_tz":-360,"elapsed":187,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["# history = modelHybrid.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,validation_data=(Xtest, Ytest), callbacks=[early_stopping])\n","\n","history = modelHybrid.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,validation_data=(Xtest, Ytest), callbacks=[clr, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tqlJKTBMz0g","executionInfo":{"status":"aborted","timestamp":1674750179834,"user_tz":-360,"elapsed":189,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["y_pred=np.argmax(modelHybrid.predict(Xtest), axis=-1)\n","y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MN79jCCWM7hg","executionInfo":{"status":"aborted","timestamp":1674750179835,"user_tz":-360,"elapsed":189,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["Ytest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6bg8KYtM_NP","executionInfo":{"status":"aborted","timestamp":1674750179840,"user_tz":-360,"elapsed":194,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["y_test=np.argmax(Ytest, axis=1)\n","y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4jeFqXENCuV","executionInfo":{"status":"aborted","timestamp":1674750179842,"user_tz":-360,"elapsed":195,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["from sklearn import metrics\n","print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n","print(metrics.confusion_matrix(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{"id":"O8E6ptNu4PCs"},"source":["#**Graph**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNhh2MZV2UES","executionInfo":{"status":"aborted","timestamp":1674750179843,"user_tz":-360,"elapsed":196,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["import pandas as pd\n","def plot_results(H):\n","    results = pd.DataFrame({\"Train Loss\": H.history['loss'], \"Validation Loss\": H.history['val_loss'],\n","              \"Train Accuracy\": H.history['accuracy'], \"Validation Accuracy\": H.history['val_accuracy']\n","             })\n","    fig, ax = plt.subplots(nrows=2, figsize=(16, 9))\n","    results[[\"Train Loss\", \"Validation Loss\"]].plot(ax=ax[0])\n","    results[[\"Train Accuracy\", \"Validation Accuracy\"]].plot(ax=ax[1])\n","\n","    \n","    ax[0].set_xlabel(\"Epoch\")\n","    ax[0].set_ylabel(\"Loss\")\n","\n","    ax[1].set_ylabel(\"Accuracy\")\n","    ax[1].set_xlabel(\"Epoch\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWSdTqF92XYB","executionInfo":{"status":"aborted","timestamp":1674750179845,"user_tz":-360,"elapsed":198,"user":{"displayName":"Sunjare Zulfiker","userId":"09299388917191134346"}}},"outputs":[],"source":["plot_results(history)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}